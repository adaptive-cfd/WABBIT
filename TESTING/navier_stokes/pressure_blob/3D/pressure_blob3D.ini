; WABBIT parameter file template.
; if you add new parameters, add them here.
; note values have to be declared "value=0;", with equal sign (=) and semicolon (;)

[Domain]
; 2D or 3D problem?
dim=3;
domain_size=6 6 3;


[Blocks]
; size of each block, should be odd (17, 33, 65 etc)
number_block_nodes=17;129;17;33;
; ghost nodes for each block
number_ghost_nodes=4;
; number of equations / components of state vector. Note you have to properly
; adjust this value for the physics module that you use.
number_equations=5;
; threshold value for thresholding wavelet coefficients
eps=1e-3;1e-4;
; treelevel bounds
max_treelevel=2;
min_treelevel=1;
; switch for mesh adaption, 1=on, ...=off
adapt_mesh=1;
; adaptive initial conditon? i.e. create grid to respect error bounds
; default is same value as adapt_mesh
adapt_inicond=1;
; in some situations, it is necessary to create the intial grid, and then refine it for a couple of times.
; for example if one does non-adaptive non-equidistant spatial convergence tests. default is 0.
inicond_refinements=0;
; block distribution for balancing (also used for start distribution)
; [equal | sfc_z | sfc_hilbert]
; equal -> simple uniformly distribution
; sfc_z  -> space filling curve -> z-curve
; sfc_hilbert -> hilbert space filling curve
block_dist=sfc_hilbert;
; non uniform mesh correction: if 1 then wabbit synchronize redundant nodes
; at coarse blocks with data from fine blocks [ 1 | ... ]
non_uniform_mesh_correction=1;


[Physics]
; what physics module is used?
; [ACM-new, ConvDiff-new, navier_stokes]
physics_type=navier_stokes;
; initial condition can be set by the physics module or read from file. in the former
; case details are given in the corrsponding subsection for each physics module.
; [ physics-module | read_from_files]
initial_cond=physics-module;
; input files that contain treecode, time, iteration, number of blocks and initial field
input_files=rho_000000000141.h5 Ux_000000000141.h5 Uy_000000000141.h5 p_000000000141.h5;



[Saving]
; TODO: maybe this section is clumsy and should be revised.
; how many fields are you going to save?
N_fields_saved=6;
; how are the fields labeled?
field_names=rho Ux Uy Uz p vort;

[Time]
; final time to reach in simulation
time_max=0.005;
; number of time steps performed. if not set, default value is very large
nt=;
; CFL criterium
CFL=0.9;
; write method (write with fixed frequency or time ) [ fixed_freq | fixed_time ]
write_method=fixed_time;
; write frequency for output, choose very large number for disabling output on disk
write_freq=;
; write time for output
write_time=0.0025;
; method to calculate time step [ fixed | CFL_cond | lvl_fixed ]

; fixed time step. if greater 0, then the time step is fixed no matter what.
; default is 0.0, so not used.
dt_fixed=;
; largest time step, if you want to set one. dt is always smaller than that, if the
; value is greater 0. default is 0.0, so not used.
dt_max=0.0;




[Navier_Stokes]
; adiabatic coefficient
gamma_=1.4;
; specific gas constant
Rs=287.05         ;for air
; prandtl number
Pr=0.71;
; dynamic viscosity
mu0=1e-2;
; dissipation, 1=on, ...=off
dissipation=1;
!; case studies of the NStokes module:
; + simple_geometry: [triangle | cylinder | rhombus]
; + funnel
; + shock_tube: [sod_shock_tube| standing_shock]
; + no: [pressure_blob | shear_layer]
case=no;


[Initial_Values]
inicond=pressure_blob;
inicond_width=0.5;
initial_pressure=101330.0;
initial_velocity=0.0 0.0 0;
initial_temperature=200;
initial_density=1.645;

[Discretization]
; order of derivatives [ FD_2nd_central | FD_4th_central_optimized ]
order_discretization=FD_4th_central_optimized;
; order of refinement predictor [ multiresolution_4th | multiresolution_2nd ]
order_predictor=multiresolution_4th;
; boundary condition [ periodic ]
boundary_cond=periodic;
; filter type [no_filter | explicit_5pt | explicit_7pt | explicit_9pt | explicit_11pt | wavelet | bogey_shock]
filter_type=no_filter;


[Timing]
; If set to 1, the code will issue a single XXXXXtimes.dat file per proc, where one
; can examine individual mpiranks manually. this file is written in every iteration.
; the IO cost on some machines can be substantial if many cores are used: better disable
; this functionality then. default is 0.
write_individual_timings=0;

[Debug]
; check if the ghost node synchronization gives the right order, on a random
; grid. this test costs some CPU time but no memory. It is done only once at startup.
test_ghost_nodes_synch=1;
test_treecode=0;
; internal testing routine for the ghost nodes: allocates HUGE amounts of memory
check_redundant_nodes=0;
